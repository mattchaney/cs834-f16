\section{Question 8.3}


\subsection{Question}
For one query in the CACM collection (provided at the book website), generate a ranking using Galago, and then calculate average precision, NDCG at 5 and 10, precision at 10, and the reciprocal rank by hand.


\subsection{Approach}
Galago version 3.10 was first downloaded from the Project Lemur Source Forge website, which can be found at the following URL: \url{https://sourceforge.net/projects/lemur/files/lemur/galago-3.10/}.  The CACM document corpus was downloaded from the textbook's website, found here: \url{http://www.search-engines-book.com/collections/}.  Galago was used to create an index of the CACM corpus and to run as a server to respond to queries on that index.

The getrel.py script (found in Listing \ref{listing:getrel}) was created to issue queries to the Galago search server using the Python Requests library \cite{py:requests}.  The HTML responses were then parsed using the Python Beautiful Soup library \cite{py:beautifulsoup}, where the CACM document identifiers were extracted for use in calculating the different evaluation scores for the Galago ranking.

The query used was from the CACM query set, number 10, and only the first 1000 retrieved documents were considered when calculating all scores for this experiment.


\subsubsection{Initial Precision and Recall Calculations}
Precision and Recall were calculated with the following equations:

\begin{equation}
\nonumber
Recall = \frac{\mid A \cap B \mid}{\mid A \mid}
\end{equation}

\begin{equation}
\nonumber
Precision = \frac{\mid A \cap B \mid}{\mid B \mid}
\end{equation}

In these equations, \(A\) is the relevant set of documents for the query, and \(B\) is the set of retrieved documents.


\subsubsection{Calculating Precision at Specific Rankings}
A list of precision values was created by calculating the cumulative precision at each document ranking with the set of retrieved documents up to that ranking.


\subsubsection{Calculating Average Precision}
Average precision was calculated by adding the precision at each retrieval ranking position for documents which are part of \(A \cap B\), or the set of retrieved documents that are relevant, and then dividing by the size of that set to obtain the average.  This can also be described as the area under the precision-recall curve, which can be expressed as the following summation:

\begin{equation}
\nonumber
AveP = \sum_{k=1}^{n} P(k)\Delta r(k)
\end{equation}

where \(k\) is the rank in the sequence of retrieved documents, \(n\) is the number of retrieved documents, \(P(k)\) is the precision at cut-off \(k\) in the list, and \(\Delta r(k)\) is the change in recall from items \(k-1\) to \(k\).


\subsubsection{Calculating Normalized Discounted Cumulative Gain (NDCG)}
First, discounted cumulative gain at rank \(p\) (\(DCG_p\)) was calculated with the following formula:

\begin{equation}
\nonumber
DCG_p = rel_1 + \sum_{i=2}^p \frac{rel_i}{log_2i}
\end{equation}

The ideal discounted cumulative gain at rank \(p\) (\(IDCG_p\)) is a simple series, expressed as:

\begin{equation}
\nonumber
IDCG_p = 1 + \sum_{i=2}^p \frac{1}{log_2i}
\end{equation}

Finally, normalized discounted cumulative gain at rank \(p\) (\(NDCG_p\)) is expressed as:

\begin{equation}
\nonumber
NDCG_p = \frac{DCG_p}{IDCG_p}
\end{equation}

with \(rel_i\) being the relevancy for document \(i\) in the retrieval ranking.  For this experiment, this value is either \(0\) or \(1\).


\subsubsection{Calculating Reciprocal Rank}
Reciprocal rank is defined as the reciprocal of the rank at which the first relevant document is found, so if the \(3^{rd}\) document in the retrieval ranking list is the first relevant document, the reciprocal rank is \(\frac{1}{3}\).


\subsection{Results}
After building the index, CACM query 10 was processed by the \texttt{getrel.py} script, the output of which can be found in Listing \ref{listing:getrelout}.  This script calculates all the values shown in Table \ref{tab:query}, which are all of the required values for the question.

\lstinputlisting[language={}, caption={Output from running the getrel.py script for queries 1 and 10 from the CACM collection.}, label=listing:getrelout]{code/getrel/getrelout.txt}

\begin{table}[h!]
\centering
\begin{tabular}{ | c | c | c | c | c | c | }
\hline
Query \# & Avg. Prec. & NDCG @5 & NDCG @10 & Prec. @10 & Recip. Rank \\
\hline
10 & 0.697677898817 & 1.0 & 0.942709999032 & 0.9 & 1.0 \\
\hline
\end{tabular}
\caption{Calculations for CACM query 10 from top 1000 retrieved documents.}
\label{tab:query}
\end{table}
