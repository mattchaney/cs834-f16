\section{Question 8.3}


\subsection{Question}
For one query in the CACM collection (provided at the book website), generate a ranking using Galago, and then calculate average precision, NDCG at 5 and 10, precision at 10, and the reciprocal rank by hand.


\subsection{Approach}
Galago version 3.10 was first downloaded from the Project Lemur Source Forge website, which can be found at \url{https://sourceforge.net/projects/lemur/files/lemur/galago-3.10/}.  The CACM document corpus was downloaded from the textbook website, found at \url{http://www.search-engines-book.com/collections/}.  Galago was used to create an index of the CACM corpus and then as a search server to respond to queries on that index.

The \texttt{getrel.py} and \texttt{q83.py} scripts (found in Listings \ref{listing:getrel} and \ref{listing:q83}, respectively) were created to issue CACM queries to the Galago search server using the Python Requests library \cite{py:requests}.  The HTML responses the server sent back were parsed using the Python Beautiful Soup library \cite{py:beautifulsoup}, where the CACM document identifiers were extracted for use in calculating the different evaluation scores for the ranking.  Various functions were added to the \texttt{getrel.py} script to derive the scores for all of the exercises in this assignment.

CACM query 10 was used for this exercise and the retrieval set was limited to the first 100 documents.


\subsubsection{Initial Precision and Recall Calculations}
Precision and Recall were calculated with the following equations:

\begin{equation}
\nonumber
Recall = \frac{\mid A \cap B \mid}{\mid A \mid}
\end{equation}

\begin{equation}
\nonumber
Precision = \frac{\mid A \cap B \mid}{\mid B \mid}
\end{equation}

Where \(A\) is the relevant set of documents for the query, and \(B\) is the set of retrieved documents.


\subsubsection{Calculating Precision at Specific Rankings}
A list of precision values was created by calculating the cumulative precision at each document ranking with the set of retrieved documents up to that ranking.


\subsubsection{Calculating Average Precision}
Average precision was calculated by adding the precision at each retrieval ranking position for documents which are part of \(A \cap B\), or the set of retrieved documents that are relevant, and then dividing by the size of that set to obtain the average.  This can also be described as the area under the precision-recall curve, which can be expressed as the following summation:

\begin{equation}
\nonumber
AveP = \sum_{k=1}^{n} P(k)\Delta r(k)
\end{equation}

where \(k\) is the rank in the sequence of retrieved documents, \(n\) is the number of retrieved documents, \(P(k)\) is the precision at cut-off \(k\) in the list, and \(\Delta r(k)\) is the change in recall from items \(k-1\) to \(k\).


\subsubsection{Calculating Normalized Discounted Cumulative Gain (NDCG)}
First, discounted cumulative gain at rank \(p\) (\(DCG_p\)) was calculated with the following formula:

\begin{equation}
\nonumber
DCG_p = rel_1 + \sum_{i=2}^p \frac{rel_i}{log_2i}
\end{equation}

with \(rel_i\) being the relevancy for document \(i\) in the retrieval ranking.  For this experiment, this value is either \(0\) or \(1\). \\

The ideal discounted cumulative gain at rank \(p\) (\(IDCG_p\)) is expressed as:

\begin{equation}
\nonumber
IDCG_p = 1 + \sum_{i=2}^p \frac{1}{log_2i}
\end{equation}

Finally, normalized discounted cumulative gain at rank \(p\) (\(NDCG_p\)) is expressed as:

\begin{equation}
\nonumber
NDCG_p = \frac{DCG_p}{IDCG_p}
\end{equation}



\subsubsection{Calculating Reciprocal Rank}
Reciprocal rank is defined as the reciprocal of the rank at which the first relevant document is found, so if the \(3^{rd}\) document in the retrieval ranking list is the first relevant document, the reciprocal rank is \(\frac{1}{3}\).


\subsection{Results}
After building the index, CACM query 10 was processed by the \texttt{getrel.py} script, the output of which can be found in Listing \ref{listing:q83out}.  This script calculates all the values shown in Table \ref{tab:q83}.

\lstinputlisting[language={}, caption={Output from running the q83.py script for query 10 from the CACM collection.}, label=listing:q83out]{code/getrel/q83out.txt}

\begin{table}[h!]
\centering
\begin{tabular}{ | c | c | c | c | c | c | }
\hline
Query \# & Avg. Prec. & NDCG @5 & NDCG @10 & Prec. @10 & Recip. Rank \\
\hline
10 & 0.5922383982 & 1.0 & 0.942709999032 & 0.9 & 1.0 \\
\hline
\end{tabular}
\caption{Calculations for CACM query 10 from all retrieved documents.}
\label{tab:q83}
\end{table}
