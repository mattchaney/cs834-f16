\section{Question 1.1}

\subsection{Question}
Think up and write down a small number of queries for a web search engine.  Make sure that the queries vary in length (i.e., they are not all one word).  Try to specify exactly what information you are looking for in some of the queries.  Run these queries on two commercial web search engines and compare the top 10 results for each query by doing relevance judgments. Write a report that answers at least the following questions: What is the precision of the results? What is the overlap between the results for the two search engines? Is one search engine clearly better than the other? If so, by how much? How do short queries perform compared to long queries?


\subsection{Resources}
The search engines Google \cite{google} and DuckDuckGo \cite{duckduckgo} were used to obtain the results.\\


\subsection{Methodology}
The following search queries were issued to each of the two mentioned search engines:

\begin{enumerate}
    \item professional skateboarder song
    \item daewoo song
    \item rollerblade song korea
    \item daewon song
\end{enumerate}

The expected relevant pages will contain information regarding the professional skateboarder named Daewon Song.  A score is assigned to each search engine for each query using the following equation:

\[\frac{R}{10}\]

Where \textit{R} the number of relevant documents returned for the given query.  Overlap is calculated by using the following equation:

\[ \frac{\| G_i \cap D_i \|}{ 10 }\]

Where \(G_i\) is the set of pages returned by Google for query \textit{i} and \(D_i\) is the set of pages returned by DuckDuckGo for query \textit{i}.  This result is divided by the size of the overall result set for each query per search engine, ten.\\

A brief description of the results is provided, with any items that stand out to the reviewer given extra measure.  After the detailed listing of results, Table \ref{tab:11comparison} lists the relevancy scores and overlap for each search engine per query.\\


First Query: ``professional skateboarder song''\\

The relevancy scores from the first query were evenly matched for both search engines, five out of ten relevant documents each.  Interestingly, Google provided more social media related results, whereas DuckDuckGo provided a couple celebrity net worth results.\\

\clearpage

Second Query: ``daewoo song''\\

Trying to throw a curveball at the engine with this one, it is intentionally misspelled.  Results for both engines show active error correction and query suggestion functionality, which helps to increase relevance when a user perhaps does not know the correct spelling of an item they wish to find.  Google wins this round with eight out of ten versus DuckDuckGo's result of 6 out of ten.\\


Third Query: ``rollerblade song korea''\\

Somewhat of another curve ball, although this is \textit{close} to a ``correct'' query, since rollerblading and skateboarding are similar ``extreme'' sports.  The results, however, show that these types of judgements are not available to a search engine, as both websites returned zero relevant documents pertaining to Daewon Song.  Perhaps the error simply lies between the chair and the keyboard.\\

Fourth Query: ``daewon song''\\

This is a control, as it is exactly the name of the target of the search query for this question.  Both search engines provided a perfect relevancy score, which was not surprising considering that the information need was embodied in a person with a unique name, making narrowing down the relevant results quite easy based on the name alone.


\subsection{Results}
Table \ref{tab:11comparison} shows the relevancy scores and overlap for each search engine per query.

\begin{table}[h!]
\centering
\begin{tabular}{| l | c | c | c |}
\hline
Query & Google Relevance & DuckDuckGo Relevance & Overlap \\
\hline
professional skateboarder song & 0.5 & 0.5 & 0.1 \\
daewoo song & 0.8 & 0.6 & 0.2 \\
rollerblade song korea & 0.0 & 0.0 & 0.0 \\
daewon song & 1.0 & 1.0 & 0.7 \\
\hline
\end{tabular}
\caption{Relevance Scores}
\label{tab:11comparison}
\end{table}

Both search engines performed relatively similarly to each other for the given queries.  Neither one provided a significantly higher number of relevant results for any of the queries.  When given the ideal query, overlap was high, although not at 1.0, while also providing a ``perfect'' relevancy score of 1.0.  The length of the query did not seem to be a strong controlling factor, term accuracy seemed to decide the relevancy of the top ten results for a particular topic.